{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# REGRESSION ASSIGNMENT\n",
        "\n",
        "- 1)What is Simple Linear Regression?\n",
        "- Ans. It is a statistical method used to model the relationship between one independent variable (X) and one dependent variable (Y) using a straight line.\n",
        " Y = mX + c\n",
        "\n",
        "- 2)What are the key assumptions of Simple Linear Regression?\n",
        "- Ans.\n",
        "\n",
        "Linearity: The relationship between X and Y is linear.\n",
        "\n",
        "Independence: Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity: Constant variance of residuals (errors).\n",
        "\n",
        "Normality: Residuals should be normally distributed.\n",
        "\n",
        "- 3) What does the coefficient m represent in the equation Y = mX + c?\n",
        "- Ans. m is the slope, representing the rate of change in Y for each unit increase in X.\n",
        "\n",
        "- 4) What does the intercept c represent in the equation Y = mX + c?\n",
        "- Ans. c is the intercept, the expected value of Y when X = 0.\n",
        "\n",
        "- 6) What is the purpose of the least squares method in Simple Linear Regression?\n",
        "- Ans. To minimize the sum of the squared differences between actual and predicted Y values, making the best-fitting line.\n",
        "\n",
        "- 7) How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "- Ans. R² indicates the proportion of variance in the dependent variable explained by the independent variable.\n",
        "For example, R² = 0.85 means 85% of the variance in Y is explained by X.\n",
        "\n",
        "- 8) What is Multiple Linear Regression?\n",
        "- Ans. It is an extension of simple linear regression that models the relationship between two or more independent variables and a dependent variable using the equation:\n",
        "Y = b₀ + b₁X₁ + b₂X₂ + ... + bₙXₙ\n",
        "\n",
        "- 9) What is the main difference between Simple and Multiple Linear Regression?\n",
        "- Ans.\n",
        "Simple: One independent variable\n",
        "\n",
        "Multiple: Two or more independent variables\n",
        "\n",
        "- 10) What are the key assumptions of Multiple Linear Regression?\n",
        "- Ans.\n",
        "\n",
        "Linearity\n",
        "\n",
        "Independence of errors\n",
        "\n",
        "Homoscedasticity\n",
        "\n",
        "Normal distribution of residuals\n",
        "\n",
        "No multicollinearity among independent variables\n",
        "\n",
        "\n",
        "- 11) What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "- Ans. Heteroscedasticity means non-constant variance of errors. It can lead to inefficient estimates and incorrect conclusions about significance.\n",
        "\n",
        "- 12) How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "- Ans.\n",
        "Remove or combine correlated predictors\n",
        "\n",
        "Use dimensionality reduction (like PCA)\n",
        "\n",
        "Use regularization methods (like Ridge or Lasso)\n",
        "\n",
        "- 13) What are some common techniques for transforming categorical variables for use in regression models?\n",
        "- Ans.\n",
        "\n",
        "One-Hot Encoding\n",
        "\n",
        "Label Encoding\n",
        "\n",
        "Ordinal Encoding\n",
        "\n",
        "\n",
        "- 14) What is the role of interaction terms in Multiple Linear Regression?\n",
        "- Ans .Interaction terms model the combined effect of two variables on the outcome, capturing dependencies between predictors.\n",
        "\n",
        "- 15)How can the interpretation of the intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "- Ans. Simple: Y when X = 0\n",
        "\n",
        "Multiple: Y when all X variables = 0, which may not always make sense or be meaningful in context\n",
        "\n",
        "- 16) What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "- Ans Each slope represents how much Y changes for a one-unit increase in the corresponding X, keeping all other variables constant.\n",
        "\n",
        "- 17) How does the intercept in a regression model provide context for the relationship between variables?\n",
        "- Ans. It provides a baseline value of Y when all independent variables are 0. It helps anchor the regression line on the graph.\n",
        "\n",
        "- 18) What are the limitations of using R² as a sole measure of model performance?\n",
        "- Ans.\n",
        "\n",
        "Doesn’t indicate whether the model is biased\n",
        "\n",
        "Can increase with more predictors, even if they’re not useful\n",
        "\n",
        "Doesn’t reflect overfitting risk\n",
        "\n",
        "- 19) How would you interpret a large standard error for a regression coefficient?\n",
        "- Ans. A large standard error suggests the coefficient is not estimated precisely, meaning it may not be statistically significant.\n",
        "\n",
        "- 20) How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "- Ans. t appears as a funnel shape in residual vs. fitted plots. It can distort test statistics, making results unreliable.\n",
        "\n",
        "- 21)  What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "- Ans. It means that some variables may not be meaningful, and the model might be overfitted by including irrelevant predictors.\n",
        "\n",
        "- 22) Why is it important to scale variables in Multiple Linear Regression?\n",
        "- Ans. To ensure variables with large ranges don’t dominate the regression model. Scaling also helps in regularization techniques.\n",
        "\n",
        "\n",
        "- 23) What is polynomial regression?\n",
        "- Ans. A type of regression where the relationship between independent and dependent variables is modeled as an nth-degree polynomial.\n",
        "\n",
        "- 24) How does polynomial regression differ from linear regression?\n",
        "- Ans.\n",
        "Linear: Relationship is a straight line\n",
        "\n",
        "Polynomial: Can model curves and nonlinear trends using powers of X\n",
        "\n",
        "- 25) When is polynomial regression used?\n",
        "- Ans. When the data shows a nonlinear relationship that can’t be captured by a straight line.\n",
        "\n",
        "- 27) Can polynomial regression be applied to multiple variables?\n",
        "- Ans. Yes, it becomes a multivariate polynomial regression, involving interaction and power terms for multiple inputs.\n",
        "\n",
        "- 28) What are the limitations of polynomial regression?\n",
        "- Ans.\n",
        "\n",
        "Prone to overfitting\n",
        "\n",
        "Poor extrapolation\n",
        "\n",
        "Becomes complex with high\n",
        "\n",
        "- 29) What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "- Ans.\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "Adjusted R²\n",
        "\n",
        "AIC/BIC scores\n",
        "\n",
        "Residual analysis\n",
        "\n",
        "\n",
        "- 30) Why is visualization important in polynomial regression?\n",
        "- Ans. It helps identify overfitting, assess model fit, and visually understand how well the curve captures the data pattern.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LjqP6uLOibbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 31) How is polynomial regression implemented in Python?\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X, y)\n"
      ],
      "metadata": {
        "id": "bDNllPBioLks"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}